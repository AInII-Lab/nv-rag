{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the Excel files\n",
    "df_jc = pd.read_excel('xxx')\n",
    "df_mg = pd.read_excel('xxx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_jc.merge(df_mg, on='Id', suffixes=('_jc', '_mg'))\n",
    "\n",
    "columns_to_rename = [f\"model_{i}_jc\" for i in range(1, 6)]\n",
    "merged_df.rename(columns={old_col: old_col.replace('_jc', '') for old_col in columns_to_rename}, inplace=True)\n",
    "\n",
    "columns_to_drop = [f\"model_{i}_mg\" for i in range(1, 6)]\n",
    "merged_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus = pd.read_excel('/Users/mariusvach/Code/python/leitlinien_rag_2_0/data/fragen/differences_jc_mg.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_ids = consensus.Id.tolist()\n",
    "\n",
    "for id in consensus_ids:\n",
    "    for rating in ['rating_model_1_jc', 'rating_model_2_jc', 'rating_model_3_jc', 'rating_model_4_jc', 'rating_model_5_jc']:\n",
    "        merged_df.loc[merged_df.Id == id, rating] = consensus.loc[consensus.Id == id, rating].values[0]\n",
    "    for rating in ['rating_model_1_mg', 'rating_model_2_mg', 'rating_model_3_mg', 'rating_model_4_mg', 'rating_model_5_mg']:\n",
    "        merged_df.loc[merged_df.Id == id, rating] = consensus.loc[consensus.Id == id, rating].values[0]\n",
    "cs = [f\"consensus_model_{x}\" for x in range(1, 6)]\n",
    "    \n",
    "merged = merged_df.merge(consensus[cs + ['Id']], on='Id', how='left')\n",
    "\n",
    "for c in [f\"model_{x}\" for x in range(1,6)]:\n",
    "    for idx in merged[merged[f\"consensus_{c}\"].isna()].index:\n",
    "        merged.at[idx, f\"consensus_{c}\"] = merged.at[idx, f\"rating_{c}_jc\"]\n",
    "\n",
    "merged = merged[~merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('xxx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})  # Increase base font size\n",
    "\n",
    "columns_of_interest = [f\"consensus_model_{i}\" for i in range(1, 6)]\n",
    "df_selected = merged[columns_of_interest]\n",
    "\n",
    "df_melted = df_selected.melt(var_name='model', value_name='rating')\n",
    "\n",
    "df_counts = df_melted.groupby('model')['rating'].value_counts().unstack(fill_value=0)\n",
    "df_percentages = df_counts.div(df_counts.sum(axis=1), axis=0) * 100\n",
    "df_counts = df_counts.reset_index()\n",
    "df_percentages = df_percentages.reset_index()\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    ax = sns.barplot(x='model', y='value', hue='rating', data=df_counts.melt(id_vars='model', var_name='rating', value_name='value'), palette=\"crest\")\n",
    "\n",
    "    plt.xlabel('Model', fontsize=16)\n",
    "    ax.set_xticklabels([\"GPT-4o w/o RAG\", \"GPT-4o w/ RAG\", \"Llama 3.1 405B Instruct Turbo\", \"Mixtral 8x22B Instruct\", \"Claude Sonnet 3.5\"], \n",
    "                       fontsize=14, rotation=20)\n",
    "    ax.set_ylim(0,100)\n",
    "    plt.ylabel('Percentage (%)', fontsize=16)\n",
    "\n",
    "\n",
    "    # Get the crest palette colors explicitly\n",
    "    palette = sns.color_palette(\"crest\", n_colors=3)\n",
    "    \n",
    "    # Create custom legend handles with palette colors\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=palette[i], label=l) \n",
    "                      for i, l in enumerate(['Wrong', 'Inaccurate', 'Correct'])]  # noqa: E741\n",
    "    \n",
    "    # Create legend with custom colored patches\n",
    "    ax.legend(handles=legend_elements, title='Rating', \n",
    "             title_fontsize=14, fontsize=12)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    bar_width = 0.8 / 3  \n",
    "    for i, model in enumerate(df_counts['model']):\n",
    "        for j, rating in enumerate([0, 1, 2]):\n",
    "            count = df_counts.iloc[i, j+1]\n",
    "            percentage = df_percentages.iloc[i, j+1]\n",
    "            x_position = i + (j - 1) * bar_width\n",
    "            ax.text(x_position, count, f'{percentage:.1f}%', \n",
    "                   ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Get the crest palette colors explicitly\n",
    "    palette = sns.color_palette(\"crest\", n_colors=3)\n",
    "    \n",
    "    # Create plot with explicit palette\n",
    "    ax = sns.barplot(x='model', y='value', hue='rating', \n",
    "                    data=df_counts.melt(id_vars='model', var_name='rating', value_name='value'), \n",
    "                    palette=palette)\n",
    "    \n",
    "    # Create custom legend handles with palette colors\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=palette[i], label=l) \n",
    "                      for i, l in enumerate(['Wrong', 'Inaccurate', 'Correct'])]  # noqa: E741\n",
    "    \n",
    "    # Create legend with custom colored patches\n",
    "    ax.legend(handles=legend_elements, title='Rating', \n",
    "             title_fontsize=14, fontsize=12)\n",
    "    \n",
    "    plt.xlabel('Model', fontsize=16)\n",
    "    ax.set_xticklabels([\"GPT-4o w/o RAG\", \"GPT-4o w/ RAG\", \"Llama 3.1 405B Instruct Turbo\", \n",
    "                        \"Mixtral 8x22B Instruct\", \"Claude Sonnet 3.5\"], \n",
    "                       fontsize=14, rotation=20)\n",
    "    ax.set_ylim(0,100)\n",
    "    plt.ylabel('Percentage of Ratings', fontsize=16)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "total_samples = 84\n",
    "\n",
    "model_results = {\n",
    "    'GPT4_wo_RAG': {'wrong': 32.9, 'inaccurate': 47.1, 'correct': 20.0},\n",
    "    'GPT4_w_RAG': {'wrong': 15.3, 'inaccurate': 27.1, 'correct': 57.6},\n",
    "    'Llama_3.1': {'wrong': 15.3, 'inaccurate': 20.0, 'correct': 64.7},\n",
    "    'Mixtral': {'wrong': 17.6, 'inaccurate': 25.9, 'correct': 56.6},\n",
    "    'Claude': {'wrong': 10.6, 'inaccurate': 18.8, 'correct': 70.6}\n",
    "}\n",
    "\n",
    "model_counts = {}\n",
    "for model, scores in model_results.items():\n",
    "    model_counts[model] = {\n",
    "        category: round(percentage * total_samples / 100)\n",
    "        for category, percentage in scores.items()\n",
    "    }\n",
    "\n",
    "model_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(p, n, confidence=0.95):\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    se = np.sqrt((p * (100 - p)) / n)\n",
    "    return np.maximum(0, p - z * se), np.minimum(100, p + z * se)\n",
    "\n",
    "def chi_square_test(model1_counts, model2_counts):\n",
    "    categories = ['wrong', 'inaccurate', 'correct']\n",
    "    observed = np.array([\n",
    "        [model1_counts[cat] for cat in categories],\n",
    "        [model2_counts[cat] for cat in categories]\n",
    "    ])\n",
    "    chi2, p_value, _, _ = stats.chi2_contingency(observed)\n",
    "    return chi2, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Model Performance Statistics (with 95% CIs):\")\n",
    "for model, scores in model_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for category, percentage in scores.items():\n",
    "        ci_lower, ci_upper = confidence_interval(percentage, total_samples)\n",
    "        print(f\"{category}: {percentage:.1f}% [{ci_lower:.1f}%, {ci_upper:.1f}%]\")\n",
    "\n",
    "print(\"\\nPairwise Statistical Comparisons (Chi-square tests):\")\n",
    "model_names = list(model_counts.keys())\n",
    "p_values = []\n",
    "comparisons = []\n",
    "chi_squares = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        model1, model2 = model_names[i], model_names[j]\n",
    "        chi2, p_value = chi_square_test(model_counts[model1], model_counts[model2])\n",
    "        comparisons.append(f\"{model1} vs {model2}\")\n",
    "        chi_squares.append(chi2)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "\n",
    "for comp, chi2, p, p_corr, rej in zip(comparisons, chi_squares, p_values, p_values_corrected, rejected):\n",
    "    print(f\"\\n{comp}:\")\n",
    "    print(f\"Chi-square: {chi2:.3f}\")\n",
    "    print(f\"Uncorrected P-value: {p:.4f}\")\n",
    "    print(f\"Bonferroni-corrected P-value: {p_corr:.4f}\")\n",
    "    print(f\"Significant at α=0.05 (after Bonferroni correction): {'YES' if rej else 'NO'}\")\n",
    "\n",
    "counts_df = pd.DataFrame(model_counts).T\n",
    "print(\"\\nCounts DataFrame:\")\n",
    "print(counts_df)\n",
    "\n",
    "percentages_df = pd.DataFrame(model_results).T\n",
    "print(\"\\nPercentages DataFrame:\")\n",
    "print(percentages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal \n",
    "\n",
    "wrong_values = [model_results[model]['wrong'] for model in model_results]\n",
    "rest_values = [model_results[model]['inaccurate'] + model_results[model]['correct'] for model in model_results]\n",
    "\n",
    "kruskal_wrong_vs_rest = kruskal(wrong_values, rest_values)\n",
    "print(\"Kruskal-Wallis Test für 'wrong' vs. rest:\")\n",
    "print(f\"H-statistic: {kruskal_wrong_vs_rest.statistic:.2f}, p-value: {kruskal_wrong_vs_rest.pvalue:.2f}\")\n",
    "\n",
    "correct_values = [model_results[model]['correct'] for model in model_results]\n",
    "rest_values = [model_results[model]['wrong'] + model_results[model]['inaccurate'] for model in model_results]\n",
    "\n",
    "kruskal_correct_vs_rest = kruskal(correct_values, rest_values)\n",
    "print(\"Kruskal-Wallis Test für 'correct' vs. rest:\")\n",
    "print(f\"H-statistic: {kruskal_correct_vs_rest.statistic:.2f}, p-value: {kruskal_correct_vs_rest.pvalue:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrong_values = [model_results[model]['wrong'] for model in model_results]\n",
    "rest_values = [model_results[model]['inaccurate'] + model_results[model]['correct'] for model in model_results]\n",
    "\n",
    "model_names = list(model_results.keys())\n",
    "p_values = []\n",
    "comparisons = []\n",
    "chi_squares = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        model1, model2 = model_names[i], model_names[j]\n",
    "        observed = np.array([\n",
    "            [wrong_values[i], rest_values[i]],\n",
    "            [wrong_values[j], rest_values[j]]\n",
    "        ])\n",
    "        chi2, p_value, _, _ = stats.chi2_contingency(observed)\n",
    "        comparisons.append(f\"{model1} vs {model2}\")\n",
    "        chi_squares.append(chi2)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "# Bonferroni-Korrektur\n",
    "rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for comp, chi2, p, p_corr, rej in zip(comparisons, chi_squares, p_values, p_values_corrected, rejected):\n",
    "    print(f\"\\n{comp}:\")\n",
    "    print(f\"Chi-square: {chi2:.3f}\")\n",
    "    print(f\"Uncorrected P-value: {p:.4f}\")\n",
    "    print(f\"Bonferroni-corrected P-value: {p_corr:.4f}\")\n",
    "    print(f\"Significant at α=0.05 (after Bonferroni correction): {'YES' if rej else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    'GPT4_wo_RAG': {'wrong': 32.9, 'inaccurate': 47.1, 'correct': 20.0},\n",
    "    'GPT4_w_RAG': {'wrong': 15.3, 'inaccurate': 27.1, 'correct': 57.6},\n",
    "    'Llama_3.1': {'wrong': 15.3, 'inaccurate': 20.0, 'correct': 64.7},\n",
    "    'Mixtral': {'wrong': 17.6, 'inaccurate': 25.9, 'correct': 56.6},\n",
    "    'Claude': {'wrong': 10.6, 'inaccurate': 18.8, 'correct': 70.6}\n",
    "}\n",
    "\n",
    "# Convert percentages to counts\n",
    "total_samples = 84\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Convert percentages to counts and create arrays for analysis\n",
    "scores = []\n",
    "groups = []\n",
    "\n",
    "# Assign scores: wrong=0, inaccurate=1, correct=2\n",
    "for model, data in models.items():\n",
    "    # Add 'wrong' responses (score 0)\n",
    "    scores.extend([0] * round(data['wrong'] * total_samples / 100))\n",
    "    groups.extend([model] * round(data['wrong'] * total_samples / 100))\n",
    "    \n",
    "    # Add 'inaccurate' responses (score 1)\n",
    "    scores.extend([1] * round(data['inaccurate'] * total_samples / 100))\n",
    "    groups.extend([model] * round(data['inaccurate'] * total_samples / 100))\n",
    "    \n",
    "    # Add 'correct' responses (score 2)\n",
    "    scores.extend([2] * round(data['correct'] * total_samples / 100))\n",
    "    groups.extend([model] * round(data['correct'] * total_samples / 100))\n",
    "\n",
    "# Perform Kruskal-Wallis H-test\n",
    "h_statistic, p_value = stats.kruskal(*[\n",
    "    np.array(scores)[np.array(groups) == model] \n",
    "    for model in models.keys()\n",
    "])\n",
    "\n",
    "print(f\"Kruskal-Wallis H-statistic: {h_statistic:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leitlinien_rag_2_0-juoIhNGw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
