{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import Settings, load_index_from_storage, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"data/internal/openai_large_index\")\n",
    "# load index\n",
    "vsi = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr = vsi.as_retriever(similarity_top_k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(question: str) -> dict:\n",
    "    r = retr.retrieve(question)\n",
    "\n",
    "    result = {}\n",
    "    result[\"question\"] = question \n",
    "    result.update({f\"chunk_{i+1}\": chunk.text for i, chunk in enumerate(r)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STROKE_FRAGEN_PATH = 'xxx'\n",
    "CAROTIS_FRAGEN_PATH = 'xxx'\n",
    "\n",
    "with open(STROKE_FRAGEN_PATH, 'r') as file:\n",
    "    stroke_fragen = file.readlines()\n",
    "    \n",
    "stroke_fragen = [f.strip() for f in stroke_fragen]\n",
    "\n",
    "with open(CAROTIS_FRAGEN_PATH, 'r') as file: \n",
    "    carotis_fragen = file.readlines()\n",
    "    \n",
    "carotis_fragen = [f.strip() for f in carotis_fragen]\n",
    "\n",
    "fragen = stroke_fragen + carotis_fragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xxx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('xxx'):\n",
    "    results = []\n",
    "    for i,frage in tqdm.tqdm(enumerate(df.frage_jc.values), total=len(df)):\n",
    "        r = chunks(frage)\n",
    "        results.append(r)\n",
    "\n",
    "    rdf = pd.DataFrame(results)\n",
    "    len(rdf)    \n",
    "    rdf.to_csv('xxx')\n",
    "else:\n",
    "    rdf = pd.read_csv('xxx')\n",
    "    len(rdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now we have the retrieved context for each question in the rdf dataframe. \n",
    "\n",
    "What I need to do now: \n",
    "- filter them down to the wrong ones.\n",
    "- think about how to visualize it nicely so that I can actually look through them and take notes.\n",
    "- I think I can use FastHTML to whip up a small web app to look through the filtered csv file and add notes to all of the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"xxx\"\n",
    "\n",
    "antworten = pd.read_csv(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antworten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices_5 = antworten.loc[antworten[\"consensus_model_5\"] == 0.0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.loc[wrong_indices_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.read_csv('xxx')\n",
    "antworten = pd.read_csv(\"xxx\")\n",
    "\n",
    "def get_wrongs(rdf, antworten, model = 1):\n",
    "    wrong_indices = antworten.loc[antworten[f\"consensus_model_{model}\"] == 0.0].index\n",
    "    return rdf.loc[wrong_indices]\n",
    "\n",
    "df = get_wrongs(rdf, antworten, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[63].frage_jc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antworten.iloc[df.iloc[2].name][\"model_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"xxx\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"model\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "df2[\"question_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"note\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    {\"id\": 0, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 1, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 2, \"category\": \"Query Design\"},\n",
    "    {\"id\": 3, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 4, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 5, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 6, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 7, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 8, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 9, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 10, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 11, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 12, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 13, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 14, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 15, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 16, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 17, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 18, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 19, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 20, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 21, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 22, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 23, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 24, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 25, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 26, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 27, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 28, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 29, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 30, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 31, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 32, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 33, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 34, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 35, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 36, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 37, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 38, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 39, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 40, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 41, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 42, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 43, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 44, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 45, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 46, \"category\": \"Reasoning\"},\n",
    "    {\"id\": 47, \"category\": \"Retrieval\"},\n",
    "    {\"id\": 48, \"category\": \"Query Design\"},\n",
    "    {\"id\": 49, \"category\": \"Retrieval\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"problem_category\"] = [o[\"category\"] for o in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"model\").problem_category.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leitlinien_rag_2_0-juoIhNGw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
